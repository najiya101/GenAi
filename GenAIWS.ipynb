{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswin-asokan/GenAI_WS/blob/main/GenAIWS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_Hq-SjnHZol",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain\n",
        "!pip install -U langchain-community\n",
        "!pip -q install transformers\n",
        "!pip -q install bitsandbytes accelerate xformers einops\n",
        "!pip -q install datasets loralib sentencepiece\n",
        "!pip -q install pypdf\n",
        "!pip -q install docx2txt\n",
        "!pip -q install sentence_transformers\n",
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2Jw7Zd6HNko"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import Docx2txtLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from google.colab import files\n",
        "from langchain import HuggingFacePipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUORS51rHnhj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import torch\n",
        "import io\n",
        "import transformers\n",
        "import tempfile\n",
        "from torch import bfloat16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vU48IQsIVm9"
      },
      "outputs": [],
      "source": [
        "subprocess.run([\"huggingface-cli\", \"login\", \"--token\", \"{{API_TOKEN}}\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRLr2-dfIDVB"
      },
      "outputs": [],
      "source": [
        "model_id = \"Trendyol/Trendyol-LLM-7b-chat-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             device_map='auto',\n",
        "                                             load_in_8bit=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f2KCtBWIJwl"
      },
      "outputs": [],
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "document = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NACXS6__IKdE"
      },
      "outputs": [],
      "source": [
        "temp_dir = tempfile.TemporaryDirectory()\n",
        "\n",
        "for filename, content in uploaded.items():\n",
        "    file_path = os.path.join(temp_dir.name, filename)\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(content)\n",
        "\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        document.extend(loader.load())\n",
        "    elif filename.endswith('.docx') or filename.endswith('.doc'):\n",
        "        loader = Docx2txtLoader(file_path)\n",
        "        document.extend(loader.load())\n",
        "    elif filename.endswith('.txt'):\n",
        "        loader = TextLoader(file_path)\n",
        "        document.extend(loader.load())\n",
        "\n",
        "\n",
        "temp_dir.cleanup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_QpwobBITar"
      },
      "outputs": [],
      "source": [
        "document_splitter = RecursiveCharacterTextSplitter( chunk_size=500, chunk_overlap=100)\n",
        "document_chunks = document_splitter.split_documents(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za-bKXC0O6Wl"
      },
      "outputs": [],
      "source": [
        " embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF8tVVqRIpVE"
      },
      "outputs": [],
      "source": [
        "vectordb = Chroma.from_documents(document_chunks, embedding=embeddings, persist_directory='./data')\n",
        "vectordb.persist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4lHbdtKJXA8"
      },
      "outputs": [],
      "source": [
        "generator = transformers.pipeline(\n",
        "    model = model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text = True,\n",
        "    task='text-generation',\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=512,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0fFyL7CJ30Y"
      },
      "outputs": [],
      "source": [
        "llm = HuggingFacePipeline(pipeline=generator)\n",
        "\n",
        "pre_prompt = \"\"\"[INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\nGenerate the next agent response by answering the question. Answer it as succinctly as possible. You are provided several documents with titles. If the answer comes from different documents please mention all possibilities in your answer and use the titles to separate between topics or domains. If you cannot answer the question from the given documents, please state that you do not have an answer.\\n\"\"\"\n",
        "prompt = pre_prompt + \"CONTEXT:\\n\\n{context}\\n\" +\"Question : {question}\" + \"[\\INST]\"\n",
        "llama_prompt = PromptTemplate(template=prompt, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "chain = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever(), combine_docs_chain_kwargs={\"prompt\": llama_prompt}, return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssBIwKRcKFn7"
      },
      "outputs": [],
      "source": [
        "chat_history = []\n",
        "\n",
        "while True:\n",
        "    query = input(\"Please enter your question (type 'exit' to quit): \")\n",
        "\n",
        "    if query.lower() == 'exit':\n",
        "        print(\"Exiting...\")\n",
        "        break\n",
        "\n",
        "    result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "    print(\"Answer:\", result['answer'])\n",
        "    chat_history.append((query, result[\"answer\"]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}